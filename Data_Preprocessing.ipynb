{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da1bc9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_datasets\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import time,json,jsonlines\n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b67b9fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WapoV4targzPath = \"Your path\"\n",
    "WapoV4targzPath = \"/home/jovyan/shared/Datasets/WashingtonPostData/WashingtonPost.v4.tar.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "175c179b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for the untar...\n",
      "Waiting for the rename...\n",
      "File renamed successfully\n",
      "File has been added in ir-dataset directory\n"
     ]
    }
   ],
   "source": [
    "def rename(WapoV4targzPath):\n",
    "    os.system(\"cp %s ./Data/\"%WapoV4targzPath)\n",
    "    print(\"Waiting for the untar...\")\n",
    "    state = os.system(\"tar xf ./Data/WashingtonPost.v4.tar.gz -C ./Data\")\n",
    "    if state != 0:\n",
    "        print(\"Something Wrong of untar.\")\n",
    "        return\n",
    "    filedir = \"./Data/WashingtonPost.v4/data/\"\n",
    "    state = os.system(\"mv %sTREC_Washington_Post_collection.v4.jl %sTREC_Washington_Post_collection.v2.jl\"%(filedir,filedir))\n",
    "    if state != 0:\n",
    "        print(\"Something Wrong of mv rename jsonl.\")\n",
    "        return\n",
    "    time.sleep(5)\n",
    "    state = os.system(\"mv ./Data/WashingtonPost.v4/ ./Data/WashingtonPost.v2/\")\n",
    "    if state != 0:\n",
    "        print(\"Something Wrong of mv rename folder.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Waiting for the rename...\")\n",
    "    state = os.system(\"cd Data && tar -czvf WashingtonPost.v2.tar.gz WashingtonPost.v2\")\n",
    "    if state != 0:\n",
    "        print(\"Something Wrong of re-tar.\")\n",
    "        return\n",
    "    try:\n",
    "        shutil.rmtree(\"./Data/WashingtonPost.v2/\")\n",
    "    except OSError as e:\n",
    "        print(\"Error: %s - %s.\" % (e.filename, e.strerror))\n",
    "    os.system(\"rm ./Data/WashingtonPost.v4.tar.gz\")\n",
    "    print(\"File renamed successfully\")\n",
    "    \n",
    "    # The source file required is WashingtonPost.v2.tar.gz.\n",
    "    # ir_datasets expects the above file to be copied/linked under ~/.ir_datasets/wapo/WashingtonPost.v2.tar.gz.\n",
    "    status = os.system(\"cp ./Data/WashingtonPost.v2.tar.gz ~/.ir_datasets/wapo/WashingtonPost.v2.tar.gz\")\n",
    "    if status == 0:\n",
    "        print(\"File has been added in ir-dataset directory\")\n",
    "\n",
    "# The current ir-dataset library doesn't support for wapov4\n",
    "# The most convinient way is to change wapov4's name to wapov2\n",
    "# https://ir-datasets.com/wapo.html\n",
    "rename(WapoV4targzPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7880f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consdict(WapoTuple):\n",
    "    DocDic = {}\n",
    "    DocDic['doc_id'] = WapoTuple[0]\n",
    "    DocDic['url'] = WapoTuple[1]\n",
    "    DocDic['title'] = WapoTuple[2]\n",
    "    DocDic['author'] = WapoTuple[3]\n",
    "    DocDic['published_date'] = WapoTuple[4]\n",
    "    DocDic['kicker'] = WapoTuple[5]\n",
    "    DocDic['body'] = WapoTuple[6]\n",
    "    DocDic['body_paras_html'] = WapoTuple[7]\n",
    "    DocDic['body_media'] = WapoTuple[8]\n",
    "    return DocDic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf92960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputpath = './Data/WapoV4_irdataset/'\n",
    "# Load dataset\n",
    "dataset = ir_datasets.load(\"wapo/v2\")\n",
    "\n",
    "def ConvertToCsv(outputpath,dataset):\n",
    "    tmppath = \"./Data/tmp/\"\n",
    "    os.mkdir(tmppath)\n",
    "    DocList = []\n",
    "    DocCount = 0\n",
    "    for doc in dataset.docs_iter():\n",
    "        DocDic = consdict(doc)\n",
    "        dumplist = [\"Opinion\",\"Opinions\",\"Letters to the Editor\",\"The Post’s View\"]\n",
    "        if DocDic['kicker'] not in dumplist:\n",
    "            DocList.append(DocDic)\n",
    "            DocCount += 1\n",
    "        if DocCount % 200==0:\n",
    "            df = pd.DataFrame(DocList)\n",
    "            df.to_csv(tmppath+'Part'+str(DocCount//200)+'.csv', index=False)\n",
    "            DocList = []\n",
    "    df = pd.DataFrame(DocList)\n",
    "    df.to_csv(tmppath+'Part'+str(DocCount//200+1)+'.csv', index=False)\n",
    "    print(\"We got %s files, %s documents!\"%(str(DocCount//200+1),str(DocCount)))\n",
    "    \n",
    "    files = os.listdir(tmppath)\n",
    "    try:\n",
    "        files.remove('.ipynb_checkpoints')\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    for file in tqdm(files):\n",
    "        try:\n",
    "            temp = pd.read_csv(tmppath+file)\n",
    "        except:\n",
    "            continue\n",
    "        temp.rename(columns={\"doc_id\":\"id\"},inplace=True)\n",
    "        temp.to_csv(outputpath+file,index=False)\n",
    "    shutil.rmtree(\"./Data/tmp/\")\n",
    "    print(\"Precessing succeeded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bdec1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] [starting] building docstore\n",
      "docs_iter: 728626doc [16:37, 730.43doc/s] \n",
      "[INFO] [finished] docs_iter: [16:37] [728626doc] [730.43doc/s]\n",
      "[INFO] [finished] building docstore [16:38]\n",
      "  0%|          | 2/3371 [00:00<03:32, 15.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 3371 files, 674050 documents!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3371/3371 [03:27<00:00, 16.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precessing succeeded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ConvertToCsv(outputpath,dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d592f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#WapoV3targzPath = \"Your path\"\n",
    "WapoV3targzPath = '/home/jovyan/shared/Datasets/WashingtonPostData/WashingtonPost.v3/data/TREC_Washington_Post_collection.v3.jl'\n",
    "v3outputpath='./Data/WapoV3/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c812237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConsDic_V3(jsonfile, dictionary):\n",
    "    dictionary['id'] = jsonfile['id']\n",
    "    dictionary['title'] = jsonfile['title']\n",
    "    dictionary['author'] = jsonfile['author']\n",
    "    \n",
    "    if 'article_url' in jsonfile:\n",
    "        dictionary['url'] = jsonfile['article_url']\n",
    "    else:\n",
    "        dictionary['url'] = None\n",
    " \n",
    "    if 'published_date' in jsonfile:\n",
    "        dictionary['published_date'] = jsonfile['published_date']\n",
    "    elif 'publish_date' in jsonfile:\n",
    "        dictionary['published_date'] = jsonfile['publish_date']\n",
    "    else:\n",
    "        dictionary['published_date'] = None\n",
    "    \n",
    "    if 'type' in jsonfile:   \n",
    "        dictionary['type'] = jsonfile['type']\n",
    "    else:\n",
    "        dictionary['type'] = None\n",
    "    \n",
    "    if 'source' in jsonfile:\n",
    "        dictionary['source'] = jsonfile['source']\n",
    "    else:\n",
    "        dictionary['source'] = None\n",
    "    \n",
    "    content = ''\n",
    "    for i in range(len(jsonfile['contents'])):\n",
    "        if jsonfile['contents'][i] != None:\n",
    "            try:\n",
    "                content+=str(jsonfile['contents'][i]['content'])\n",
    "            except KeyError:\n",
    "                pass\n",
    "    dictionary['content'] = content\n",
    "    \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a7db8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "671947it [03:48, 2945.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We got 336 files, 671947 documents!\n"
     ]
    }
   ],
   "source": [
    "def ConvertToCsv_v3(WapoV3targzPath,v3outputpath):\n",
    "    with open(WapoV3targzPath,'r',encoding='UTF-8') as f:\n",
    "        DocList = []\n",
    "        DocCount = 0\n",
    "        for line in tqdm(f):\n",
    "            doc = json.loads(line)\n",
    "            DocDic = {}\n",
    "\n",
    "            DocDic = ConsDic_V3(doc,DocDic)\n",
    "\n",
    "            DocList.append(DocDic)\n",
    "            DocCount += 1\n",
    "            if DocCount % 2000==0:\n",
    "                df = pd.DataFrame(DocList)\n",
    "                df.to_csv(v3outputpath+'Part'+str(DocCount//2000)+'.csv', index=False)\n",
    "                DocList = []\n",
    "        df = pd.DataFrame(DocList)\n",
    "        df.to_csv(v3outputpath+'Part'+str(DocCount//2000+1)+'.csv', index=False)\n",
    "        print(\"We got %s files, %s documents!\"%(str(DocCount//2000+1),str(DocCount)))\n",
    "        \n",
    "ConvertToCsv_v3(WapoV3targzPath,v3outputpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0f88fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reformat topic file\n",
    "# append doc information\n",
    "# This step should after document expansion\n",
    "\n",
    "path_to_topic2021 = './Data/Topic/trec2021/newsir21-topics.txt'\n",
    "path_to_topic2020 = './Data/Topic/trec2020/newsir20-topics.txt'\n",
    "\n",
    "def mergedoc_queries(path_to_topic,version):\n",
    "    #replace with your jsonl file\n",
    "    washingtonpath = '~/TREC_Washington_Post_collection.v4.jl'\n",
    "    \n",
    "    with open(path_to_topic,\"r+\") as f:\n",
    "        content = f.read()\n",
    "        content = content.replace(\"<data>\",\"\")\n",
    "        content = content.replace(\"</data>\",\"\")\n",
    "        f.seek(0,0)\n",
    "        f.write(\"<data>\\n\" + content+ \"\\n</data>\")\n",
    "    \n",
    "    tree = ET.parse(path_to_topic)\n",
    "    root = tree.getroot()\n",
    "    \n",
    "    print(\"There are \",len(root),\" topics.\")\n",
    "    \n",
    "    if version == \"2021\":\n",
    "        num, docid, url, title, desc, narr, subtopics = [],[],[],[],[],[],[]\n",
    "        for i in range(len(root)):\n",
    "            sub = []\n",
    "            num.append(root[i][0].text.strip())\n",
    "            docid.append(root[i][1].text.strip())\n",
    "            url.append(root[i][2].text.strip())\n",
    "            title.append(root[i][3].text.strip())\n",
    "            desc.append(root[i][4].text.strip())\n",
    "            narr.append(root[i][5].text.strip())\n",
    "            for j in range(len(root[i][6])):\n",
    "                sub.append(root[i][6][j].text.strip())\n",
    "            subtopics.append(sub)\n",
    "        \n",
    "        dict = {'num': num, 'docid': docid, 'url':url, 'title':title, 'desc': desc, 'narr': narr, 'subtopics': subtopics} \n",
    "        df = pd.DataFrame(dict)\n",
    "        \n",
    "        with open(washingtonpath,'r',encoding='UTF-8') as f:\n",
    "            DocList = []\n",
    "            DocCount = 0\n",
    "            for line in tqdm(f):\n",
    "                doc = json.loads(line)\n",
    "                DocDic = {}\n",
    "                DocDic = ConsDic_V3(doc,DocDic)\n",
    "                if DocDic['id'] in df['docid'].values:\n",
    "                    DocList.append(DocDic)\n",
    "                    DocCount += 1\n",
    "        df2 = pd.DataFrame(DocList)\n",
    "        df2.rename(columns={'content': 'qcontent','id':'docid'},inplace=True)\n",
    "        query_df = pd.merge(df,df2,on='docid')\n",
    "        print(\"We merged %s topics.\"%DocCount)\n",
    "        \n",
    "        files = os.listdir('./Data/WapoV4_expanded/')\n",
    "        \n",
    "        try:\n",
    "            files.remove('.ipynb_checkpoints')\n",
    "        except:\n",
    "            pass\n",
    "        docidlist = query_df['docid'].tolist()\n",
    "        query_df['Query'] = ''\n",
    "        query_df['Key_Words'] = ''\n",
    "        for f in tqdm(files):\n",
    "            temp = pd.read_csv('./Data/WapoV4_expanded/'+f)\n",
    "            for i in range(len(temp)):\n",
    "                if temp['id'][i] in docidlist:\n",
    "                    query_df['Query'].loc[query_df['docid']==temp['id'][i]] = temp['Query'][i]\n",
    "                    query_df['Key_Words'].loc[query_df['docid']==temp['id'][i]] = temp['Key_Words'][i]\n",
    "        print(\"expand extracted keywords and predicted queries to topic.\")\n",
    "        query_df.to_csv(\"./Data/Topic/trec2021/topicquery.csv\",index=False)\n",
    "        print(\"Done\")\n",
    "    \n",
    "    elif version==\"2020\":\n",
    "        num, docid, url = [],[],[]\n",
    "        for i in range(len(root)):\n",
    "            num.append(root[i][0].text.strip())\n",
    "            docid.append(root[i][1].text.strip())\n",
    "            url.append(root[i][2].text.strip())\n",
    "        topicdict = {'num': num, 'docid': docid, 'url':url} \n",
    "        df = pd.DataFrame(topicdict)\n",
    "  \n",
    "        with open(washingtonpath,'r',encoding='UTF-8') as f:\n",
    "            DocList = []\n",
    "            DocCount = 0\n",
    "            for line in f:\n",
    "                doc = json.loads(line)\n",
    "                DocDic = {}\n",
    "                DocDic = ConsDic_V3(doc,DocDic)\n",
    "                if DocDic['id'] in df['docid'].values:\n",
    "                    DocList.append(DocDic)\n",
    "                    DocCount += 1\n",
    "        df2 = pd.DataFrame(DocList)\n",
    "        print(\"We merged %s topics.\"%DocCount)\n",
    "        df2.rename(columns={'id': 'docid'},inplace=True)\n",
    "        query_df = pd.merge(df,df2,on='docid')\n",
    "        \n",
    "        files = os.listdir('./Data/WapoV4_expanded/')\n",
    "        \n",
    "        try:\n",
    "            files.remove('.ipynb_checkpoints')\n",
    "        except:\n",
    "            pass\n",
    "        docidlist = query_df['docid'].tolist()\n",
    "        query_df['Query'] = ''\n",
    "        query_df['Key_Words'] = ''\n",
    "        for f in tqdm(files):\n",
    "            temp = pd.read_csv('./Data/WapoV4_expanded/'+f)\n",
    "            for i in range(len(temp)):\n",
    "                if temp['id'][i] in docidlist:\n",
    "                    query_df['Query'].loc[query_df['docid']==temp['id'][i]] = temp['Query'][i]\n",
    "                    query_df['Key_Words'].loc[query_df['docid']==temp['id'][i]] = temp['Key_Words'][i]\n",
    "        print(\"expand extracted keywords and predicted queries to topic.\")\n",
    "        query_df.to_csv(\"./Data/Topic/trec2020/topicquery.csv\",index=False)\n",
    "        print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "614783a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "735it [00:00, 7348.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  51  topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "728626it [01:33, 7797.33it/s] \n",
      "  0%|          | 7/3218 [00:00<00:50, 63.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We merged 51 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3218/3218 [00:47<00:00, 67.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand extracted keywords and predicted queries to topic.\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mergedoc_queries(path_to_topic2021,\"2021\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8372c0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  50  topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 7/3218 [00:00<00:50, 62.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We merged 50 topics.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3218/3218 [00:47<00:00, 68.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "expand extracted keywords and predicted queries to topic.\n",
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mergedoc_queries(path_to_topic2020,\"2020\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed6c122",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
